{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_frames(video_path, interval=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    prev_hist = None\n",
    "    key_frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Uniform sampling\n",
    "        if frame_count % interval == 0:\n",
    "            key_frames.append(frame)\n",
    "\n",
    "        # Shot bounday detection \n",
    "        hist = cv2.calcHist([frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "        if prev_hist is not None:\n",
    "            hist_diff = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL)\n",
    "            if hist_diff < 0.5:  # Threshold\n",
    "                key_frames.append(frame)\n",
    "\n",
    "        prev_hist = hist\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return key_frames\n",
    "\n",
    "def get_image_embeddings(key_frames, model, preprocess, device):\n",
    "    image_embeddings = []\n",
    "\n",
    "    for frame in tqdm(key_frames):\n",
    "        # Convert OpenCV image (BGR) to PIL image (RGB)\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Preprocess the image\n",
    "        preprocessed_image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get the image embedding\n",
    "        with torch.no_grad():\n",
    "            image_embedding = model.encode_image(preprocessed_image)\n",
    "        \n",
    "        image_embeddings.append(image_embedding.cpu().numpy())\n",
    "\n",
    "    image_embeddings_tensor = torch.tensor(np.vstack(image_embeddings))\n",
    "    return image_embeddings_tensor\n",
    "\n",
    "def get_text_embedding(text, model, device):\n",
    "    # Preprocess the input text\n",
    "    text_inputs = clip.tokenize([text]).to(device)\n",
    "    \n",
    "    # Get the text embedding\n",
    "    with torch.no_grad():\n",
    "        text_embedding = model.encode_text(text_inputs)\n",
    "    \n",
    "    return text_embedding.cpu()\n",
    "\n",
    "def cosine_sim(x, y):\n",
    "    x = x / x.norm(dim=1, keepdim=True)\n",
    "    y = y / y.norm(dim=1, keepdim=True)\n",
    "    return x @ y.T # dot prod\n",
    "\n",
    "def top_k_similarity(image_embeddings, text_embedding, k=5):\n",
    "    similarities = cosine_sim(image_embeddings, text_embedding)\n",
    "    if k == 1:\n",
    "        top_k_indices = torch.topk(similarities, k, dim=0).indices.item()\n",
    "        return [top_k_indices]\n",
    "    else:\n",
    "        top_k_indices = torch.topk(similarities, k, dim=0).indices.squeeze().cpu().numpy()\n",
    "        return top_k_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_path, text_prompt, interval=30, k=1):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    key_frames = extract_key_frames(video_path, interval=interval)\n",
    "    image_embeddings = get_image_embeddings(key_frames, model, preprocess, device)\n",
    "    text_embedding = get_text_embedding(text_prompt, model, device)\n",
    "    top_k_indices = top_k_similarity(image_embeddings, text_embedding, k=k)\n",
    "    print(top_k_indices)\n",
    "    top_k_frames = [key_frames[i] for i in top_k_indices]\n",
    "    for i, frame in enumerate(top_k_frames):\n",
    "        cv2.imwrite(f'top_frame_{i}.jpg', frame)\n",
    "    return top_k_frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:11<00:00, 12.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136]\n"
     ]
    }
   ],
   "source": [
    "top_k_frames = main(\"test2.mp4\", \"lava\")\n",
    "for i, frame in enumerate(top_k_frames):\n",
    "    cv2.imwrite(f'top_frame_{i}.jpg', frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
